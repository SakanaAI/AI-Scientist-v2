[
    {
        "Name": "automated_concept_sae_eval",
        "Title": "Extending Principled SAE Evaluations to Complex Tasks via Automated Concept Discovery",
        "Short Hypothesis": "Automated concept discovery methods can generate 'pseudo-supervised' feature dictionaries for complex tasks, enabling the principled evaluation framework (sufficiency, necessity, control) from Makelov et al. to be applied beyond templatic scenarios.",
        "Related Work": "Builds directly on the principled evaluation framework proposed by Makelov et al. (2025). Leverages automated concept discovery techniques potentially inspired by works like Representation Engineering (Zou et al., 2023), PACE (Luo et al., 2024), or clustering/dimensionality reduction on activations. Contrasts with purely indirect SAE evaluation metrics (Bricken et al., 2023) or evaluations based on pre-defined human concepts applied to specific domains (Gao et al., 2024; Chaudhary & Geiger, 2024).",
        "Abstract": "Evaluating sparse autoencoders (SAEs) for LLM interpretability is hampered by the lack of ground truth features, especially on complex, non-templatic tasks. Makelov et al. (2025) proposed a principled evaluation by comparing SAEs to supervised feature dictionaries derived from known task attributes, demonstrated on the structured IOI task. However, this relies on manual attribute identification. We propose to overcome this limitation by using automated concept discovery methods (e.g., clustering, sparse PCA on activations, contrastive probing) to identify salient operational concepts directly from model activations on more complex linguistic tasks (e.g., sentiment analysis, simple QA). These automatically discovered concepts will be used to generate 'pseudo-supervised' feature dictionaries. We hypothesize that these dictionaries can serve as effective benchmarks within the Makelov et al. framework, allowing quantitative evaluation of SAE sufficiency, necessity, and controllability concerning these emergent concepts. This extends principled SAE evaluation to scenarios where human-defined attributes are unavailable or insufficient, providing a more scalable approach to assessing SAE utility on realistic tasks.",
        "Experiments": [
            "Select a suitable complex linguistic task (e.g., sentiment classification on IMDB, a simple closed-book QA dataset) and a base LLM (e.g., GPT-2 Small/Medium).",
            "Implement and apply an automated concept discovery technique (e.g., sparse PCA on residual stream activations, k-means clustering on activation differences between contrasting inputs) to identify a set of potential 'concept vectors' or salient activation directions relevant to the task.",
            "Validate the meaningfulness of discovered concepts (e.g., via activation patching effect on loss, correlation with specific input features if available, automated interpretation methods).",
            "Compute 'pseudo-supervised' feature dictionaries using the mean feature dictionary method (Makelov et al.) based on the activation patterns associated with the discovered concepts (treating concept presence/absence or intensity as the 'attribute').",
            "Train various SAE architectures (e.g., vanilla, Gated, TopK) on the same model activations used for concept discovery.",
            "Evaluate the trained SAEs against the pseudo-supervised dictionaries using the sufficiency, necessity, and sparse control metrics from Makelov et al., adapted to the automatically discovered concepts.",
            "Analyze whether the framework successfully differentiates SAE quality on the complex task and provides interpretable insights into how SAEs represent the discovered concepts."
        ],
        "Risk Factors and Limitations": [
            "The quality, stability, and interpretability of automatically discovered concepts may be limited or highly dependent on the chosen discovery method and hyperparameters.",
            "The generated 'pseudo-supervised' dictionaries might not capture the task's core mechanisms as effectively as human-defined attributes in simpler tasks, potentially weakening the evaluation baseline.",
            "Applying control/editing metrics might be difficult if the discovered concepts are abstract or lack clear semantic meaning.",
            "Computational cost associated with analyzing large activation datasets and running concept discovery algorithms.",
            "The evaluation framework still fundamentally relies on the linear representation hypothesis for the pseudo-supervised dictionaries."
        ]
    }
]